{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Categorical-Preprocessing-\" data-toc-modified-id=\"Categorical-Preprocessing--1\">Categorical Preprocessing </a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Categorical-Feature-Engineering-\" data-toc-modified-id=\"Categorical-Feature-Engineering--3\">Categorical Feature Engineering </a></span></li><li><span><a href=\"#What-are-the-most-common-ways-to-encode-categorical-features?\" data-toc-modified-id=\"What-are-the-most-common-ways-to-encode-categorical-features?-4\">What are the most common ways to encode categorical features?</a></span></li><li><span><a href=\"#What-is-the-difference-between-pd.get_dummies-and-sklearn.preprocessing.OneHotEncoder?\" data-toc-modified-id=\"What-is-the-difference-between-pd.get_dummies-and-sklearn.preprocessing.OneHotEncoder?-5\">What is the difference between pd.get_dummies and sklearn.preprocessing.OneHotEncoder?</a></span></li><li><span><a href=\"#Embeddings-(e.g.,-word2vec)\" data-toc-modified-id=\"Embeddings-(e.g.,-word2vec)-6\">Embeddings (e.g., word2vec)</a></span></li><li><span><a href=\"#One-hot-encoding-vs-Embedding\" data-toc-modified-id=\"One-hot-encoding-vs-Embedding-7\">One-hot encoding vs Embedding</a></span></li><li><span><a href=\"#One-way-to-use-word-embeddings-in-a-ML-Algorithm\" data-toc-modified-id=\"One-way-to-use-word-embeddings-in-a-ML-Algorithm-8\">One way to use word embeddings in a ML Algorithm</a></span></li><li><span><a href=\"#StarSpace-Package:-Embed-All-The-Things!\" data-toc-modified-id=\"StarSpace-Package:-Embed-All-The-Things!-9\">StarSpace Package: Embed All The Things!</a></span></li><li><span><a href=\"#How-to-use-StarSpace-embeddings-in-a-ML-Algorithm\" data-toc-modified-id=\"How-to-use-StarSpace-embeddings-in-a-ML-Algorithm-10\">How to use StarSpace embeddings in a ML Algorithm</a></span></li><li><span><a href=\"#Comparing-Encoding-Methods\" data-toc-modified-id=\"Comparing-Encoding-Methods-11\">Comparing Encoding Methods</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-12\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-13\">Bonus Material</a></span></li><li><span><a href=\"#Bin-Counting\" data-toc-modified-id=\"Bin-Counting-14\">Bin Counting</a></span></li><li><span><a href=\"#Bin-Counting\" data-toc-modified-id=\"Bin-Counting-15\">Bin Counting</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-16\">Check for understanding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Categorical Preprocessing </h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/0_iKsDex5fUBQoYTju.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- List the common methods of encoding categorical features.\n",
    "- Compare and contrast the common methods of encoding categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Categorical Feature Engineering </h2></center>\n",
    "\n",
    "Categorical variables needs to be transformed into numbers that amendable to machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples of categorical variables: Products, users, words, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the most common ways to encode categorical features?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One-Hot Encoding / Dummy Encoding\n",
    "- Embeddings \n",
    "\n",
    "[category_encoders](https://github.com/scikit-learn-contrib/category_encoders) package has many techniques that all with scikit-learn.\n",
    "\n",
    "For the sake of time, I'm only go to cover a couple of techniques to show how the scikit-learn API works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the difference between pd.get_dummies and sklearn.preprocessing.OneHotEncoder?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pd.get_dummies` creates a DataFrame to manage encoded variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`sklearn.preprocessing.OneHotEncoder` creates an OneHotEncoder object to manage encoded variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(['Positive', 'Positive', 'Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Negative  Positive\n",
       "0         0         1\n",
       "1         0         1\n",
       "2         1         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.get_dummies(pd.Series(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_Negative', 'x0_Positive'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(X.reshape(-1, 1))\n",
    "enc.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result is similar how ever it is a numpy array ready for machine learning\n",
    "enc.transform(X.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine fit and transform\n",
    "enc.fit_transform(X.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Embeddings (e.g., word2vec)</h2></center>\n",
    "\n",
    "<center><img src=\"../images/0_A9qy9wS7m-eMKiX6.png\" width=\"45%\"/></center>\n",
    "\n",
    "<center>Project sparse high-dimensional categorical data into a dense, low-dimensional numeric vector.</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://instagram-engineering.com/emojineering-part-1-machine-learning-for-emoji-trendsmachine-learning-for-emoji-trends-7f5f9cb979ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>One-hot encoding vs Embedding</h2></center>\n",
    "\n",
    "The advantages of embedding is converting categorical variables to numeric variables. Those numerical variables have semantic meaning. Thus any valid mathematical operation for numerical values has interpretable meaning.\n",
    "\n",
    "One-hot encoding just tells the model that two entities are different. Embedding tells how different the entities are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://datascience.stackexchange.com/questions/71808/when-can-embeddings-be-useful-for-small-input-spaces/82517#82517)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>One way to use word embeddings in a ML Algorithm</h2></center>\n",
    "\n",
    "1. Train (or download) word embeddings.\n",
    "1. Create a document embedding by taking the an average of words vectors for that document.\n",
    "1. Each dimension is a feature.\n",
    "1. Train the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, tree-based classifier would learn which parts of the embedding space are associated with a specific label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>StarSpace Package: Embed All The Things!</h2></center>\n",
    "\n",
    "<center><img src=\"../images/product_embedding.png\" width=\"75%\"/></center>\n",
    "\n",
    "[StarSpace](https://github.com/facebookresearch/StarSpace) can embedded any sequential discrete data. \n",
    "\n",
    "Examples: Words, emojis, documents, users, products, images, videos, …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How to use StarSpace embeddings in a ML Algorithm</h2></center>\n",
    "\n",
    "1. Train (or download) embeddings where every entity is embedded into the same space.\n",
    "1. Each dimension is a feature.\n",
    "1. Train the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \n",
    "\n",
    "- https://research.fb.com/downloads/starspace/\n",
    "- https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "     font-size: 100%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
    "     font-size: 100%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Comparing Encoding Methods</h2></center>\n",
    "\n",
    "| Method | Size Requirements |  Growth |\n",
    "|:-------:|:------:|:------:|\n",
    "| One-Hot Encoding | Cardinality of data| Unbounded |\n",
    "| Feature Hashing | Hash table size | Fixed | \n",
    "| Embeddings | Feature space dimensions | Fixed | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- One-hot encoding works well for small number of categories\n",
    "- Features hashing works well for medium number of categories.\n",
    "- Embeddings are the best choice (if possible).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do create categorical __targets__?\n",
    "\n",
    "LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit_transform(['cat', 'cat', 'dog', 'llama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bin Counting\n",
    "-------\n",
    "\n",
    "<center><img src=\"../images/counts.png\" width=\"75%\"/></center>\n",
    "\n",
    "Rather than using the value of the categorical variable as the feature, instead use the __conditional probability of the target under that value__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bin Counting\n",
    "-----\n",
    "\n",
    "<center><img src=\"../images/bin_conts.png\" width=\"75%\"/></center>\n",
    "\n",
    "Use probability of click as a feature (not just raw click or not click).\n",
    "\n",
    "Turns a large, sparse, binary representation of the categorical variable (e.g., one-hot encoding) into a very small, dense, real-valued numeric representation\n",
    "\n",
    "Bin Counting: _m_ bin table size, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the difference between Bin Counting and Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Naive Bayes always multiplies the conditional probabilities.\n",
    "\n",
    "Bin counting treat them as features, which can be used in other models such as trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Learn more:\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/beyond-one-hot-17-ways-of-transforming-categorical-features-into-numeric-features-57f54f199ea4\n",
    "- https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63\n",
    "- https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\n",
    "- https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\n",
    "- https://blogs.technet.microsoft.com/machinelearning/2015/02/17/big-learning-made-easy-with-counts/\n",
    "- https://www.slideshare.net/SessionsEvents/misha-bilenko-principal-researcher-microsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
