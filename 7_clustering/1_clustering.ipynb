{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-1\">Clustering</a></span></li><li><span><a href=\"#By-The-End-Of-This-Session-You-Should-Be-Able-To:\" data-toc-modified-id=\"By-The-End-Of-This-Session-You-Should-Be-Able-To:-2\">By The End Of This Session You Should Be Able To:</a></span></li><li><span><a href=\"#What-is-clustering?\" data-toc-modified-id=\"What-is-clustering?-3\">What is clustering?</a></span></li><li><span><a href=\"#Why-is-Clustering-Useful?\" data-toc-modified-id=\"Why-is-Clustering-Useful?-4\">Why is Clustering Useful?</a></span></li><li><span><a href=\"#The-number-of-clusters-can-be-ambiguous\" data-toc-modified-id=\"The-number-of-clusters-can-be-ambiguous-5\">The number of clusters can be ambiguous</a></span></li><li><span><a href=\"#2-major-kinds-of-clustering\" data-toc-modified-id=\"2-major-kinds-of-clustering-6\">2 major kinds of clustering</a></span></li><li><span><a href=\"#Common-Types-of-Clustering-Algorithms\" data-toc-modified-id=\"Common-Types-of-Clustering-Algorithms-7\">Common Types of Clustering Algorithms</a></span></li><li><span><a href=\"#Less-Common-Types-of-Clustering-Algorithms\" data-toc-modified-id=\"Less-Common-Types-of-Clustering-Algorithms-8\">Less Common Types of Clustering Algorithms</a></span></li><li><span><a href=\"#Clustering-in-scikit-learn\" data-toc-modified-id=\"Clustering-in-scikit-learn-9\">Clustering in scikit-learn</a></span></li><li><span><a href=\"#Clustering-needs-a-measure-of-similarity\" data-toc-modified-id=\"Clustering-needs-a-measure-of-similarity-10\">Clustering needs a measure of similarity</a></span></li><li><span><a href=\"#Choosing-a-similarity-measurement-is-often-more-important-than-clustering-algorithm\" data-toc-modified-id=\"Choosing-a-similarity-measurement-is-often-more-important-than-clustering-algorithm-11\">Choosing a similarity measurement is often more important than clustering algorithm</a></span></li><li><span><a href=\"#Types-of-Data\" data-toc-modified-id=\"Types-of-Data-12\">Types of Data</a></span></li><li><span><a href=\"#What-Are-Common-Similarity-Measures?\" data-toc-modified-id=\"What-Are-Common-Similarity-Measures?-13\">What Are Common Similarity Measures?</a></span></li><li><span><a href=\"#I-have-categorical-data.-How-do-I-cluster-it?\" data-toc-modified-id=\"I-have-categorical-data.-How-do-I-cluster-it?-14\">I have categorical data. How do I cluster it?</a></span></li><li><span><a href=\"#I-have-mixed-data-(continuous-and-categorical).-How-do-I-cluster-it?\" data-toc-modified-id=\"I-have-mixed-data-(continuous-and-categorical).-How-do-I-cluster-it?-15\">I have mixed data (continuous and categorical). How do I cluster it?</a></span></li><li><span><a href=\"#You-should-collect-distance-metrics-(like-you-should-be-collecting-evaluation-metrics).\" data-toc-modified-id=\"You-should-collect-distance-metrics-(like-you-should-be-collecting-evaluation-metrics).-16\">You should collect distance metrics (like you should be collecting evaluation metrics).</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-17\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-18\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-19\">Bonus Material</a></span></li><li><span><a href=\"#What-about-&quot;Big-Data&quot;?\" data-toc-modified-id=\"What-about-&quot;Big-Data&quot;?-20\">What about \"Big Data\"?</a></span></li><li><span><a href=\"#Common-Options-to-Cluster-&quot;Big-Data&quot;\" data-toc-modified-id=\"Common-Options-to-Cluster-&quot;Big-Data&quot;-21\">Common Options to Cluster \"Big Data\"</a></span></li><li><span><a href=\"#Evaluating-Clustering\" data-toc-modified-id=\"Evaluating-Clustering-22\">Evaluating Clustering</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Clustering</h2></center>\n",
    "<br>\n",
    "<center><img src=\"../images/machine_clustering.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define clustering in your words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is clustering?</h2></center>\n",
    "\n",
    "<center><img src=\"../images/clusters.png\" width=\"85%\"/></center>\n",
    "\n",
    "A unsupervised machine learning method that learns to partition unlabeled data into groups of similar data objects.\n",
    "\n",
    "A cluster is a group of data objects.\n",
    "\n",
    "The notion of a \"cluster\" cannot be precisely defined (which is one of the reasons why there are so many clustering algorithms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/inter_distances.png\" width=\"75%\"/></center>\n",
    "\n",
    "Items in a group are more similar to each other than than other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is Clustering Useful?</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "Raw data is often too complex for humans to understand, clustering can help to reduce that complexity.\n",
    "\n",
    "Clustering can help with:\n",
    "\n",
    "- Understanding - What the heck is going on in my data?\n",
    "- Grouping data for subsequent analysis - Intermediate step in a data pipeline.\n",
    "- Summarization - Since these items are similar, describing one describes the others.\n",
    "- Recommendation - If you like this item, you might like similar items.\n",
    "- Lossy data compression - See example from 621 about color palette in images\n",
    "\n",
    "For example, Topic Modeling is clustering similar documents together. Then humans only have to look a couple of example from each cluster to better understand the entire corpus and each group can be easily summarized.\n",
    "\n",
    "Clustering can be useful in exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>The number of clusters can be ambiguous</h2></center>\n",
    "\n",
    "Student Poll\n",
    "<center><img src=\"../images/many.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/2_clusters.png\" width=\"75%\"/></center>\n",
    "<center><img src=\"../images/4_clusters.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center><img src=\"../images/6.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>2 major kinds of clustering</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Hard / exclusive / non-fuzzy clustering\n",
    "\n",
    "    Each data point is assigned to one and only cluster.\n",
    "<br><br>\n",
    "- Soft / non-exclusive / fuzzy clustering\n",
    "\n",
    "    Each data point can belong to multiple clusters, often with a probability of membership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Common Types of Clustering Algorithms</h2></center>\n",
    "\n",
    "- Centroid models: Each cluster by represented a single mean vector (k-means)\n",
    "- Connectivity models: Based on distance connectivity (hierarchical clustering)\n",
    "- Distribution models: Modeled using statistical distributions (Gaussian Mixture Model (GMM))\n",
    "- Graph-based models: Modeled as edges between nodes (Spectral Clustering)\n",
    "- Density models: Connected dense regions (DBSCAN)\n",
    "- Subspace models: Use both cluster members and relevant attributes ( biclustering)\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Less Common Types of Clustering Algorithms</h2></center>\n",
    "\n",
    "- Grid-based models\n",
    "- Group models\n",
    "- Neural networks models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Clustering in scikit-learn</h2></center>\n",
    "\n",
    "Let's read the documentation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Clustering needs a measure of similarity</h2></center>\n",
    "\n",
    "Similarity can be thought of as the \"inverse of distance\".      \n",
    "\n",
    "There is no single complete definition of a similarity measure.\n",
    "\n",
    "Similarity measures should:\n",
    "\n",
    "- Have larger values for similar objects. \n",
    "- Have smaller values for very dissimilar objects, including zero or a negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Choosing a similarity measurement is often more important than clustering algorithm</h2></center>\n",
    "\n",
    "__What are you really trying to do?__\n",
    "\n",
    "> Specifying an appropriate dissimilarity measure is far more important in obtaining success with clustering than the choice of clustering algorithm.  \n",
    "> — The Elements of Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Types of Data</h2></center>\n",
    "\n",
    "<center><img src=\"../images/types of data.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center><img src=\"../images/do with data.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What Are Common Similarity Measures?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Discrete metrics \n",
    "    - binary\n",
    "        - Contingency table\n",
    "        - Hamming Distance\n",
    "    - Levenshtein - edit distance\n",
    "    - [Sørensen–Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) - set data (aka, anything hashable) \n",
    "    - [Jaccard](https://en.wikipedia.org/wiki/Jaccard_index) - Set data (aka, anything hashable) \n",
    "- Continuous metrics \n",
    "    - Cosine similarity\n",
    "    - Minkowski distance ($L^1$, $L^2$, …)\n",
    "- Distributional metrics \n",
    "    - Gaussian - univariate, bivariate, multivariate\n",
    "    - Mahalanobis - between a point and a distribution\n",
    "    - K-L Divergence - information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>I have categorical data. How do I cluster it?</h2></center>\n",
    "\n",
    "Can you convert it to numerical data. For example, with embeddings?\n",
    "\n",
    "If not, try k-modes (instead of k-means).\n",
    "\n",
    "K-modes defines clusters based on the number of matching categories between data points. (This is in contrast to the more well-known k-means algorithm, which clusters numerical data based on Euclidean distance.) \n",
    "\n",
    "There is a scikit-learn-ish package for that [k-modes](https://pypi.org/project/kmodes/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>I have mixed data (continuous and categorical). How do I cluster it?</h2></center>\n",
    "\n",
    "Again, can embedding it. Embedding all your data in single space is very useful.\n",
    "\n",
    "Otherwise:\n",
    "\n",
    "1. k-prototypes algorithm\n",
    "\n",
    "    The k-prototypes algorithm combines k-modes and k-means and is able to cluster mixed numerical / categorical data. \n",
    "    \n",
    "    See [k-modes](https://pypi.org/project/kmodes/) package.\n",
    "<br><br>\n",
    "2. Grower's Distance\n",
    "\n",
    "    Read the original paper - [A General Coefficient of Similarity and Some of Its Properties](http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Gower1971general.pdf))   \n",
    "    Grower's Distance is coming to scikit-learn [here](https://github.com/scikit-learn/scikit-learn/pull/16834)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is sample implementation\n",
    "# It will not handle all edge cases and errors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "def gower_distance(X:pd.DataFrame) -> np.array:\n",
    "    \"\"\"Calucate a  distance matrix will be returned which will contain the pairwise gower distance between the rows\n",
    "\n",
    "    All columns of object type will be treated as nominal variables.\n",
    "    All other columns will be treated as  numeric variables.\n",
    "\n",
    "    Distance metrics used for:\n",
    "        Nominal variables: Dice distance (https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n",
    "        Numeric variables: Manhattan distance normalized by the range of the variable (https://en.wikipedia.org/wiki/Taxicab_geometry)\n",
    "    \"\"\"\n",
    "    individual_variable_distances = []\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        feature = X.iloc[:,[i]]\n",
    "        if feature.dtypes[0] == np.object:\n",
    "            feature_dist = DistanceMetric.get_metric('dice').pairwise(pd.get_dummies(feature))\n",
    "        else:\n",
    "            feature_dist = DistanceMetric.get_metric('manhattan').pairwise(feature) / np.ptp(feature.values)\n",
    "\n",
    "        individual_variable_distances.append(feature_dist)\n",
    "\n",
    "    return np.array(individual_variable_distances).mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code Source](https://stats.stackexchange.com/questions/444593/how-to-use-gowers-distance-with-dbscan-algorithm-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>You should collect distance metrics (like you should be collecting evaluation metrics).</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Clustering is an unsupervised machine learning technique that tries to group similar datapoints together.\n",
    "- Clustering needs a measure of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- [What is Cluster Analysis?](https://slideplayer.com/slide/17397475/)\n",
    "- https://towardsdatascience.com/the-k-prototype-as-clustering-algorithm-for-mixed-data-type-categorical-and-numerical-fe7c50538ebb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about \"Big Data\"?\n",
    "-----\n",
    "\n",
    "Data can be big in many ways. There can be many rows or columns (or even high cardinality).\n",
    "\n",
    "[On the Surprising Behavior of Distance Metrics in High Dimensional Space](https://link.springer.com/chapter/10.1007/3-540-44503-X_27)\n",
    "\n",
    "\n",
    "Common Options to Cluster \"Big Data\"\n",
    "----\n",
    "\n",
    "- Reduce dimensionality then cluster\n",
    "    - Embeddings\n",
    "    - PCA\n",
    "- Switch to [approximate nearest neighbor](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods) methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/Contingency table.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Clustering\n",
    "-----\n",
    "\n",
    "Let's read the documentation  \n",
    "\n",
    "- scikit-learn\n",
    "    - [Overview](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.cluster) \n",
    "    - [Deeper dive](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)\n",
    "- beyond scikit-learn\n",
    "    - https://en.wikipedia.org/wiki/Cluster_analysis#Evaluation_and_assessment\n",
    "    - https://neptune.ai/blog/the-ultimate-guide-to-evaluation-and-selection-of-models-in-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
